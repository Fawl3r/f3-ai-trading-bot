#!/usr/bin/env python3
"""
Live-Ops Setup Script for Advanced Learning Layer
Orchestrates monitoring, alerts, and automation for production deployment
"""

import os
import sqlite3
import subprocess
import logging
import time
import json
import yaml
from pathlib import Path
from datetime import datetime
import argparse
from typing import Dict

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LiveOpsSetup:
    """
    Complete setup for Live-Ops monitoring and automation
    """
    
    def __init__(self, policy_sha: str):
        self.policy_sha = policy_sha
        self.setup_time = datetime.now()
        
        # Paths and configs
        self.base_dir = Path.cwd()
        self.models_dir = self.base_dir / "models"
        self.config_dir = self.base_dir / "config"
        self.logs_dir = self.base_dir / "logs"
        
        # Create directories
        for directory in [self.models_dir, self.config_dir, self.logs_dir]:
            directory.mkdir(exist_ok=True)
        
        logger.info(f"üöÄ Live-Ops Setup initialized for policy: {policy_sha}")
    
    def verify_policy_exists(self) -> bool:
        """Verify the policy exists in the bandit database"""
        
        db_path = self.models_dir / "policy_bandit.db"
        if not db_path.exists():
            logger.error(f"‚ùå Policy database not found: {db_path}")
            return False
        
        try:
            with sqlite3.connect(str(db_path)) as conn:
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT p.id, p.name, ba.traffic_allocation, ba.total_trades
                    FROM policies p
                    JOIN bandit_arms ba ON p.id = ba.policy_id
                    WHERE p.name LIKE ?
                    ORDER BY p.created_at DESC LIMIT 1
                ''', (f'%{self.policy_sha[-8:]}%',))
                
                policy = cursor.fetchone()
                
                if policy:
                    policy_id, name, traffic, trades = policy
                    logger.info(f"‚úÖ Policy found: {name}")
                    logger.info(f"üìä Current traffic: {traffic:.1%}, Trades: {trades}")
                    return True
                else:
                    logger.error(f"‚ùå Policy not found: {self.policy_sha}")
                    return False
                    
        except Exception as e:
            logger.error(f"‚ùå Database error: {e}")
            return False
    
    def setup_prometheus_monitoring(self) -> bool:
        """Setup Prometheus monitoring and alerts"""
        
        logger.info("üìä Setting up Prometheus monitoring...")
        
        try:
            # Generate alerts configuration
            from prometheus_alerts import PrometheusAlertsManager
            
            alerts_manager = PrometheusAlertsManager()
            
            # Save alerts config
            alerts_file = self.config_dir / "prometheus_alerts.yml"
            alerts_manager.save_alerts_config(str(alerts_file))
            
            # Save Alertmanager config
            alertmanager_config = alerts_manager.generate_alertmanager_config()
            alertmanager_file = self.config_dir / "alertmanager.yml"
            
            with open(alertmanager_file, 'w') as f:
                yaml.dump(alertmanager_config, f, default_flow_style=False, indent=2)
            
            # Test Prometheus connection
            if alerts_manager.validate_prometheus_connection():
                logger.info("‚úÖ Prometheus connection validated")
            else:
                logger.warning("‚ö†Ô∏è  Prometheus not accessible, alerts config saved for later")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Prometheus setup failed: {e}")
            return False
    
    def setup_continuous_monitoring(self) -> bool:
        """Setup continuous monitoring service"""
        
        logger.info("üîÑ Setting up continuous monitoring...")
        
        try:
            # Create monitoring service script
            monitor_script = self.base_dir / "monitor_service.py"
            
            monitor_content = f'''#!/usr/bin/env python3
"""
Continuous monitoring service for policy {self.policy_sha}
Auto-generated by Live-Ops Setup
"""

import sys
import logging
from live_ops_monitor import LiveOpsMonitor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    monitor = LiveOpsMonitor()
    
    try:
        logger.info("üîÑ Starting continuous monitoring...")
        monitor.run_continuous_monitoring("{self.policy_sha}", check_interval=300)
    except KeyboardInterrupt:
        logger.info("üëã Monitoring service stopped")
    except Exception as e:
        logger.error(f"‚ùå Monitor error: {{e}}")
        sys.exit(1)

if __name__ == "__main__":
    main()
'''
            
            with open(monitor_script, 'w') as f:
                f.write(monitor_content)
            
            monitor_script.chmod(0o755)
            
            logger.info(f"‚úÖ Monitor service created: {monitor_script}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Monitor setup failed: {e}")
            return False
    
    def setup_cron_jobs(self) -> bool:
        """Setup cron jobs for automated tasks"""
        
        logger.info("‚è∞ Setting up cron jobs...")
        
        try:
            # Traffic scaling cron (nightly at 2 AM)
            scaling_cron = f"0 2 * * * cd {self.base_dir} && python live_ops_monitor.py --policy-sha {self.policy_sha} --mode scale >> {self.logs_dir}/scaling.log 2>&1"
            
            # Weekly maintenance cron (Sundays at 3 AM)
            maintenance_cron = f"0 3 * * 0 cd {self.base_dir} && python weekly_maintenance.py --task full >> {self.logs_dir}/maintenance.log 2>&1"
            
            # Model health check cron (every 6 hours)
            health_cron = f"0 */6 * * * cd {self.base_dir} && python live_ops_monitor.py --policy-sha {self.policy_sha} --mode report >> {self.logs_dir}/health.log 2>&1"
            
            # Create crontab file
            crontab_file = self.config_dir / "live_ops_crontab"
            
            with open(crontab_file, 'w') as f:
                f.write("# Live-Ops Cron Jobs for Advanced Learning Layer\n")
                f.write("# Generated automatically - do not edit manually\n\n")
                f.write("# Nightly traffic scaling\n")
                f.write(scaling_cron + "\n\n")
                f.write("# Weekly maintenance\n")
                f.write(maintenance_cron + "\n\n")
                f.write("# Health checks every 6 hours\n")
                f.write(health_cron + "\n")
            
            logger.info(f"‚úÖ Cron jobs configured: {crontab_file}")
            logger.info("üí° To install: crontab config/live_ops_crontab")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Cron setup failed: {e}")
            return False
    
    def create_deployment_manifest(self) -> bool:
        """Create deployment manifest with all configurations"""
        
        logger.info("üìã Creating deployment manifest...")
        
        try:
            manifest = {
                'deployment_info': {
                    'policy_sha': self.policy_sha,
                    'setup_time': self.setup_time.isoformat(),
                    'version': '1.0.0',
                    'environment': 'production'
                },
                'monitoring': {
                    'prometheus_alerts': 'config/prometheus_alerts.yml',
                    'alertmanager_config': 'config/alertmanager.yml',
                    'monitoring_service': 'monitor_service.py',
                    'check_interval_seconds': 300
                },
                'automation': {
                    'traffic_scaling': {
                        'enabled': True,
                        'schedule': 'daily 2:00 AM',
                        'max_traffic_share': 0.60,
                        'scaling_factor': 1.5
                    },
                    'weekly_maintenance': {
                        'enabled': True,
                        'schedule': 'Sunday 3:00 AM',
                        'tasks': ['snapshot', 'retrain', 'optuna', 'purge', 'cleanup']
                    }
                },
                'thresholds': {
                    'pf_challenger_30': {'min': 2.1, 'max': 2.8, 'flag_below': 1.6},
                    'dd_challenger_pct': {'max': 3.0, 'flag_above': 4.0},
                    'encoder_kl_divergence': {'min': 0.0, 'max': 0.25, 'flag_above': 0.30},
                    'gpu_util_pct': {'min': 2.0, 'max': 10.0, 'flag_above': 60.0}
                },
                'emergency_procedures': {
                    'throttle_threshold': 'pf_challenger_30 < 1.6',
                    'halt_threshold': 'dd_challenger_pct > 4.0',
                    'auto_revert': True,
                    'emergency_contacts': ['trading-ops@company.com']
                },
                'file_locations': {
                    'policy_database': 'models/policy_bandit.db',
                    'model_files': 'models/',
                    'log_directory': 'logs/',
                    'config_directory': 'config/'
                }
            }
            
            manifest_file = self.config_dir / "live_ops_manifest.json"
            
            with open(manifest_file, 'w') as f:
                json.dump(manifest, f, indent=2)
            
            logger.info(f"‚úÖ Deployment manifest created: {manifest_file}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Manifest creation failed: {e}")
            return False
    
    def create_monitoring_dashboard_config(self) -> bool:
        """Create Grafana dashboard configuration"""
        
        logger.info("üìä Creating monitoring dashboard config...")
        
        try:
            dashboard = {
                'dashboard': {
                    'id': None,
                    'title': f'Elite Trading - Live Ops Monitor - {self.policy_sha[-8:]}',
                    'tags': ['trading', 'live-ops', 'challenger'],
                    'timezone': 'UTC',
                    'refresh': '30s',
                    'time': {
                        'from': 'now-6h',
                        'to': 'now'
                    },
                    'panels': [
                        {
                            'id': 1,
                            'title': 'Live-Ops Checklist Status',
                            'type': 'table',
                            'gridPos': {'h': 8, 'w': 24, 'x': 0, 'y': 0},
                            'targets': [
                                {
                                    'expr': 'elite_challenger_profit_factor_30',
                                    'legendFormat': 'PF (30 trades)',
                                    'refId': 'A'
                                },
                                {
                                    'expr': 'elite_challenger_drawdown_pct',
                                    'legendFormat': 'Drawdown %',
                                    'refId': 'B'
                                },
                                {
                                    'expr': 'elite_encoder_kl_divergence',
                                    'legendFormat': 'KL Divergence',
                                    'refId': 'C'
                                },
                                {
                                    'expr': 'elite_gpu_utilization_pct',
                                    'legendFormat': 'GPU Util %',
                                    'refId': 'D'
                                }
                            ]
                        },
                        {
                            'id': 2,
                            'title': 'Traffic Allocation Over Time',
                            'type': 'timeseries',
                            'gridPos': {'h': 6, 'w': 12, 'x': 0, 'y': 8},
                            'targets': [
                                {
                                    'expr': 'elite_traffic_allocation_percent',
                                    'legendFormat': 'Challenger Traffic %'
                                }
                            ]
                        },
                        {
                            'id': 3,
                            'title': 'Performance Metrics',
                            'type': 'timeseries',
                            'gridPos': {'h': 6, 'w': 12, 'x': 12, 'y': 8},
                            'targets': [
                                {
                                    'expr': 'elite_win_rate_percent',
                                    'legendFormat': 'Win Rate %'
                                },
                                {
                                    'expr': 'elite_profit_factor',
                                    'legendFormat': 'Profit Factor'
                                }
                            ]
                        },
                        {
                            'id': 4,
                            'title': 'Active Alerts',
                            'type': 'alertlist',
                            'gridPos': {'h': 6, 'w': 24, 'x': 0, 'y': 14}
                        }
                    ]
                }
            }
            
            dashboard_file = self.config_dir / "grafana_dashboard.json"
            
            with open(dashboard_file, 'w') as f:
                json.dump(dashboard, f, indent=2)
            
            logger.info(f"‚úÖ Grafana dashboard config created: {dashboard_file}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Dashboard config creation failed: {e}")
            return False
    
    def run_initial_health_check(self) -> Dict[str, any]:
        """Run initial health check to verify everything is working"""
        
        logger.info("ü©∫ Running initial health check...")
        
        try:
            from live_ops_monitor import LiveOpsMonitor
            
            monitor = LiveOpsMonitor()
            report = monitor.generate_monitoring_report(self.policy_sha)
            
            # Save health check report
            health_file = self.logs_dir / f"initial_health_check_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            with open(health_file, 'w') as f:
                json.dump(report, f, indent=2)
            
            logger.info(f"‚úÖ Health check completed: {health_file}")
            
            # Log summary
            metrics = report.get('metrics', {})
            alerts = report.get('alerts', [])
            
            logger.info("üìä Health Check Summary:")
            logger.info(f"  Trades: {metrics.get('trades_count', 0)}")
            logger.info(f"  PF: {metrics.get('pf_challenger_30', 0):.2f}")
            logger.info(f"  DD: {metrics.get('dd_challenger_pct', 0):.1f}%")
            logger.info(f"  Alerts: {len(alerts)}")
            
            return report
            
        except Exception as e:
            logger.error(f"‚ùå Health check failed: {e}")
            return {}
    
    def generate_deployment_summary(self) -> str:
        """Generate deployment summary and next steps"""
        
        summary = f"""
üöÄ LIVE-OPS DEPLOYMENT COMPLETE

Policy SHA: {self.policy_sha}
Setup Time: {self.setup_time.strftime('%Y-%m-%d %H:%M:%S')}

üìÅ Generated Files:
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus_alerts.yml    # Prometheus alerts configuration
‚îÇ   ‚îú‚îÄ‚îÄ alertmanager.yml         # Alertmanager configuration
‚îÇ   ‚îú‚îÄ‚îÄ live_ops_crontab         # Cron jobs for automation
‚îÇ   ‚îú‚îÄ‚îÄ live_ops_manifest.json   # Complete deployment manifest
‚îÇ   ‚îî‚îÄ‚îÄ grafana_dashboard.json   # Grafana dashboard config
‚îú‚îÄ‚îÄ monitor_service.py           # Continuous monitoring service
‚îî‚îÄ‚îÄ logs/                        # Log directory for all services

üîÑ Next Steps:

1. Start Monitoring Service:
   python monitor_service.py

2. Install Cron Jobs:
   crontab config/live_ops_crontab

3. Setup Prometheus (if not running):
   - Copy config/prometheus_alerts.yml to Prometheus rules directory
   - Restart Prometheus to load new rules

4. Setup Alertmanager (if not running):
   - Copy config/alertmanager.yml to Alertmanager config directory
   - Configure SMTP settings for email alerts

5. Import Grafana Dashboard:
   - Import config/grafana_dashboard.json into Grafana
   - Configure data sources if needed

üìä Live-Ops Checklist (First 150 trades):

Metric                  | Expected Band | Flag If...  | Action
------------------------|---------------|-------------|------------------
pf_challenger_30        | 2.1 - 2.8     | < 1.6       | Throttle to 0%
dd_challenger_pct       | < 3%          | > 4%        | Auto-halt & revert
encoder_kl_divergence   | 0 - 0.25      | > 0.30      | Schedule refresh
gpu_util_pct           | 2 - 10%       | > 60%       | Restart learner

ü§ñ Automation Schedule:
- Traffic Scaling: Daily at 2:00 AM
- Health Checks: Every 6 hours
- Weekly Maintenance: Sundays at 3:00 AM

üö® Emergency Procedures:
- Automatic throttling when PF < 1.6
- Automatic halt when DD > 4%
- Revert to production baseline
- Alert notifications via email/webhook

‚úÖ System is ready for production deployment!
        """
        
        return summary.strip()
    
    def run_complete_setup(self) -> bool:
        """Run complete Live-Ops setup"""
        
        logger.info("üèóÔ∏è  Starting complete Live-Ops setup...")
        
        # Step 1: Verify policy
        if not self.verify_policy_exists():
            return False
        
        # Step 2: Setup monitoring
        if not self.setup_prometheus_monitoring():
            logger.warning("‚ö†Ô∏è  Prometheus setup had issues, continuing...")
        
        # Step 3: Setup continuous monitoring
        if not self.setup_continuous_monitoring():
            return False
        
        # Step 4: Setup automation
        if not self.setup_cron_jobs():
            return False
        
        # Step 5: Create deployment manifest
        if not self.create_deployment_manifest():
            return False
        
        # Step 6: Create dashboard config
        if not self.create_monitoring_dashboard_config():
            return False
        
        # Step 7: Run health check
        health_report = self.run_initial_health_check()
        
        # Step 8: Generate summary
        summary = self.generate_deployment_summary()
        
        # Save summary to file
        summary_file = self.base_dir / "LIVE_OPS_DEPLOYMENT_SUMMARY.md"
        with open(summary_file, 'w') as f:
            f.write(summary)
        
        # Print summary
        print("\n" + "="*80)
        print(summary)
        print("="*80)
        
        logger.info(f"üìã Full summary saved: {summary_file}")
        
        return True

def main():
    """Main setup function"""
    parser = argparse.ArgumentParser(description='Live-Ops Setup for Advanced Learning Layer')
    parser.add_argument('--policy-sha', required=True, 
                       help='Policy SHA to setup monitoring for')
    parser.add_argument('--quick-setup', action='store_true',
                       help='Run quick setup without full health checks')
    
    args = parser.parse_args()
    
    setup = LiveOpsSetup(args.policy_sha)
    
    if args.quick_setup:
        logger.info("‚ö° Running quick setup...")
        success = (
            setup.verify_policy_exists() and
            setup.setup_continuous_monitoring() and
            setup.create_deployment_manifest()
        )
    else:
        success = setup.run_complete_setup()
    
    if success:
        logger.info("üéâ Live-Ops setup completed successfully!")
        return 0
    else:
        logger.error("‚ùå Live-Ops setup failed!")
        return 1

if __name__ == "__main__":
    exit(main()) 